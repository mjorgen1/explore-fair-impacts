{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.evaluation_utils import evaluating_model\n",
    "from scripts.visualization_utils import visual_label_dist, visual_scores_by_race\n",
    "from scripts.classification_utils import load_args,prep_data,get_classifier, get_new_scores, add_constraint_and_evaluate,add_values_in_dict, save_dict_in_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/testing/Demo-0-Lab-0.csv'# path to the dataset csv-file\n",
    "results_path = '../results/demo-0-lab-0/' # directory to save the results\n",
    "\n",
    "weight_idx = 1 # weight index for samples (1 in our runs)\n",
    "testset_size = 0.3 # proportion of testset samples in the dataset (e.g. 0.3)\n",
    "test_set_variant = 0 # 0= default (testset like trainset), 1= balanced testset, 2= original,true FICO distribution\n",
    "test_set_bound = 30000 # absolute upper bound for test_set size\n",
    "\n",
    "di_means = [100,-100] # means for delayed impact distributions (rewardTP,penaltyFP)\n",
    "di_stds = [15,15] # standard deviations for delayed impact distributions (rewardTP,penaltyFP)\n",
    "\n",
    "save = True # indicator if the results should be saved\n",
    "\n",
    "models = {'Decision Tree': 'dt', 'Gaussian Naive Bayes':'gnb','Logistic Regression': 'lgr', 'Gradient Boosted Trees': 'gbt'}\n",
    "model_name = models['Gradient Boosted Trees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. IMPORTANT--Specify classifier to be trained and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{results_path}{model_name}', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results_dict = {}\n",
    "black_results_dict = {}\n",
    "white_results_dict = {}\n",
    "all_types = []\n",
    "all_scores = []\n",
    "scores_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data[['score', 'race']] = data[['score', 'race']].astype(int)\n",
    "x = data[['score', 'race']].values\n",
    "y = data['repay_indices'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_scores_by_race(results_path,'all',x)\n",
    "visual_label_dist(results_path,'all',x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, race_train, race_test, sample_weight_train, sample_weight_test = prep_data(data, testset_size,test_set_variant,test_set_bound, weight_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram of credit scores by race\n",
    "visual_scores_by_race(results_path,'train',X_train)\n",
    "visual_scores_by_race(results_path,'test',X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_label_dist(results_path,'train',X_train, y_train)\n",
    "visual_label_dist(results_path,'test',X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_b = []\n",
    "X_test_w = []\n",
    "y_test_b = []\n",
    "y_test_w = []\n",
    "\n",
    "\n",
    "for index in range(len(X_test)):\n",
    "    if race_test[index] == 0:  # black\n",
    "        X_test_b.append(X_test[index][0])\n",
    "        y_test_b.append(y_test[index])\n",
    "    elif race_test[index] == 1:  # white\n",
    "        X_test_w.append(X_test[index][0])\n",
    "        y_test_w.append(y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores and types to list\n",
    "T_test_b = ['TP' if e==1 else \"TN\" for e in y_test_b]\n",
    "T_test_w = ['TP' if e==1 else \"TN\" for e in y_test_w]\n",
    "all_types.extend([T_test_b,T_test_w])\n",
    "all_scores.extend([X_test_b,X_test_w])\n",
    "scores_names.extend(['testB', 'testW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train unmitigated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The classifier trained below is: ', model_name)\n",
    "\n",
    "results_path += f'{model_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier and collect predictions\n",
    "NOTE: atm sample_weight are all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "np.random.seed(0)\n",
    "\n",
    "# Train the classifier:\n",
    "model = classifier.fit(X_train,y_train, sample_weight_train)\n",
    "\n",
    "# Make predictions with the classifier:\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# Scores on test set\n",
    "test_scores = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unmit_b, X_unmit_w,T_unmit_b, T_unmit_w = get_new_scores(X_test, y_predict, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_unmit_b,T_unmit_w])\n",
    "all_scores.extend([X_unmit_b,X_unmit_w])\n",
    "scores_names.extend(['unmitB', 'unmitW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of unmitigated classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_str = 'Un-'\n",
    "results_overall, results_black, results_white = evaluating_model(constraint_str,X_test,y_test, y_predict, di_means,di_stds, sample_weight_test,race_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} Unmitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding Fairness Constraints - Train mitigated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint_and_evaluate(model, 'DP', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False,di_means,di_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egDP_b, X_egDP_w,T_egDP_b, T_egDP_w = get_new_scores(X_test, y_pred_mitigated, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_egDP_b, T_egDP_w])\n",
    "all_scores.extend([X_egDP_b, X_egDP_w])\n",
    "scores_names.extend(['egdpB', 'egdpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint_and_evaluate(model, 'EO', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False, di_means,di_stds,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egEO_b, X_egEO_w,T_egEO_b, T_egEO_w = get_new_scores(X_test, y_pred_mitigated, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_egEO_b, T_egEO_w])\n",
    "all_scores.extend([X_egEO_b, X_egEO_w])\n",
    "scores_names.extend(['egeoB', 'egeoW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOO (True Positive Rate Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint_and_evaluate(model, 'TPRP', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False, di_means,di_stds,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egEOO_b, X_egEOO_w,T_egEOO_b, T_egEOO_w = get_new_scores(X_test, y_pred_mitigated, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_egEOO_b, T_egEOO_w])\n",
    "all_scores.extend([X_egEOO_b, X_egEOO_w])\n",
    "scores_names.extend(['egeooB', 'egeooW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EOO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint_and_evaluate(model, 'FPRP', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False, di_means,di_stds,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egFPRP_b, X_egFPRP_w,T_egFPRP_b, T_egFPRP_w = get_new_scores(X_test, y_pred_mitigated, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_egFPRP_b, T_egFPRP_w])\n",
    "all_scores.extend([X_egFPRP_b, X_egFPRP_w])\n",
    "scores_names.extend(['egfprpB', 'egfprpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} FPRP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint_and_evaluate(model, 'ERP', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False, di_means,di_stds,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egERP_b, X_egERP_w,T_egERP_b, T_egERP_w = get_new_scores(X_test, y_pred_mitigated, y_test, di_means, di_stds, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_types.extend([T_egERP_b, T_egERP_w])\n",
    "all_scores.extend([X_egERP_b, X_egERP_w])\n",
    "scores_names.extend(['egerpB', 'egerpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_key = f'{model_name} ERP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save results to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use below!!\n",
    "if save == True:\n",
    "    overall_fieldnames = ['Run', 'Acc', 'ConfMatrix','F1micro', 'F1weighted','F1binary', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DIB','DIW', 'DP Diff', 'EO Diff', 'TPR Diff', 'FPR Diff', 'ER Diff']\n",
    "    byrace_fieldnames = ['Run', 'Acc', 'ConfMatrix','F1micro', 'F1weighted','F1binary', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DI']\n",
    "    save_dict_in_csv(overall_results_dict, overall_fieldnames,  results_path+model_name+'_overall_results.csv')\n",
    "    save_dict_in_csv(black_results_dict, byrace_fieldnames,  results_path+model_name+'_black_results.csv')\n",
    "    save_dict_in_csv(white_results_dict, byrace_fieldnames,  results_path+model_name+'_white_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    # Save overall score results\n",
    "    columns_data_scores = zip_longest(*all_scores)\n",
    "    columns_data_types = zip_longest(*all_types)\n",
    "\n",
    "    with open(results_path+model_name+'_all_scores.csv',mode='w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(scores_names)\n",
    "            writer.writerows(columns_data_scores)\n",
    "            f.close()\n",
    "    with open(results_path+model_name+'_all_types.csv',mode='w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(scores_names)\n",
    "        writer.writerows(columns_data_types)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
