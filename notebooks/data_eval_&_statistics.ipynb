{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns;\n",
    "\n",
    "from scripts.evaluation_utils import delayed_impact_csv, immediate_impact_csv, delayed_impact_german_csv,types_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../final_results/fico_data/syn_orig/ii/'\n",
    "folders= ['dt','gnb','lgr','gbt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build useful CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 FP/TP/TN/FN Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_csvs(data_path, folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  0 \n",
      " DataFrame: \n",
      "                DT   GNB   LGR   GBT\n",
      "Constraint                         \n",
      "Unmitigated  0.10  3.50 -7.11  0.02\n",
      "DP           0.62 -4.57 -7.40 -0.13\n",
      "EO          -0.44 -1.97 -5.52  0.35\n",
      "EOO          0.35 -8.30 -6.91 -0.21\n",
      "FPER        -1.10 -5.21 -6.45 -0.11\n",
      "ERP         -0.27  2.85 -6.31  0.08\n",
      "Group:  1 \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  37.04  24.87  36.16  37.18\n",
      "DP           37.37  18.07  38.57  37.50\n",
      "EO           37.03  34.30  33.96  36.99\n",
      "EOO          38.61  36.38  36.66  38.68\n",
      "FPER         37.62  36.08  36.28  37.47\n",
      "ERP          33.57  27.17  31.29  33.92\n"
     ]
    }
   ],
   "source": [
    "delayed_impact_csv(data_path,0, folders)\n",
    "delayed_impact_csv(data_path,1, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  0 \n",
      " DataFrame: \n",
      "                DT    GNB    LGR   GBT\n",
      "Constraint                           \n",
      "Unmitigated  0.49  0.472  0.529  0.49\n",
      "DP           0.49  0.503  0.536  0.49\n",
      "EO           0.49  0.505  0.526  0.49\n",
      "EOO          0.49  0.530  0.532  0.49\n",
      "FPER         0.49  0.525  0.529  0.49\n",
      "ERP          0.49  0.472  0.529  0.49\n",
      "Group:  1 \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  0.647  0.743  0.672  0.643\n",
      "DP           0.508  0.311  0.585  0.505\n",
      "EO           0.569  0.583  0.599  0.565\n",
      "EOO          0.573  0.607  0.615  0.571\n",
      "FPER         0.635  0.647  0.665  0.631\n",
      "ERP          0.573  0.737  0.599  0.585\n"
     ]
    }
   ],
   "source": [
    "immediate_impact_csv(data_path,0, folders)\n",
    "immediate_impact_csv(data_path,1, folders)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delayed_impact_german_csv(data_path,0, folders)\n",
    "delayed_impact_german_csv(data_path,1, folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analyzing Scores (only fico_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Extracting Scores from csv into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores Data Frames\n",
    "classifier_dfs = {}\n",
    "dfs_b = {}\n",
    "dfs_w = {}\n",
    "# loading test set credit scores into dictinary from all models\n",
    "for f in folders:\n",
    "    path = f'{data_path}{f}/{f}_all_scores.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.round(0)\n",
    "\n",
    "    df_black = df.filter(like='B')\n",
    "    df_white = df.filter(like='W')\n",
    "    \n",
    "    classifier_dfs[f] = df\n",
    "    dfs_b[f] = df_black\n",
    "    dfs_w[f] = df_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checking for normal distributions:\n",
    "\n",
    "if p < 0.01 (or < 0.05) then the distribution is significantly different from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: dt\n",
      "Check for norm Distributions done\n",
      "Classifier: gnb\n",
      "Check for norm Distributions done\n",
      "Classifier: lgr\n",
      "Check for norm Distributions done\n",
      "Classifier: gbt\n",
      "Check for norm Distributions done\n"
     ]
    }
   ],
   "source": [
    "for c,df in classifier_dfs.items():\n",
    "    print('Classifier:',c)\n",
    "    for col in df:\n",
    "        data=df[col].dropna(axis=0)\n",
    "        _,p = stats.kstest(data, \"norm\") # comparing score distribution to normal distribution\n",
    "        if p > 0.01:\n",
    "            print(col,',p:',p)\n",
    "    print('Check for normal distributions->done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Significance of Score Distributions with Mann Whitney U test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_path = f'{data_path}mwu/'\n",
    "os.makedirs(mwu_path,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of Distributions unmitigated v mitigated for each race\n",
    "\n",
    "if p < 0.001 (or < 0.0005) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_race_mwu(dfs, b_or_w = 'B'):\n",
    "    p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "    p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "    for c,df in dfs.items():\n",
    "        \n",
    "        c = f'{c}{b_or_w}'\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data_unmiti = df[f'unmit{b_or_w}'].dropna(axis=0) # extract scores from unmitigated models\n",
    "        df = df.iloc[:,1:]\n",
    "        for col in df:\n",
    "            \n",
    "            idx.append(col[:-1])\n",
    "            \n",
    "            data_miti=df[col].dropna(axis=0) # extract scores from mitigated models\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data_unmiti, data_miti) # compare unmitigated and mitigated models\n",
    "            col_vals.append(p)\n",
    "            \n",
    "            # p values < 0.05 the difference is significat\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c] = col_signi\n",
    "        p_vals[c] = col_vals\n",
    "    # set index\n",
    "    p_vals['Constraints'] = idx\n",
    "    p_vals = p_vals.set_index('Constraints')\n",
    "    \n",
    "    p_signi['Constraints'] = idx\n",
    "    p_signi = p_signi.set_index('Constraints')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    # save p_values and significance for all models\n",
    "    p_vals.to_csv(f'{mwu_path}p_un_vs_miti_{b_or_w}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_un_vs_miti_{b_or_w}.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black:\n",
      "            dtB gnbB lgrB gbtB\n",
      "Constraints                   \n",
      "unmit                         \n",
      "dp                 s          \n",
      "eo                 s          \n",
      "tprp               s          \n",
      "fprp               s          \n",
      "erp                           \n",
      "\n",
      "White:\n",
      "            dtW gnbW lgrW gbtW\n",
      "Constraints                   \n",
      "unmit                         \n",
      "dp                 s          \n",
      "eo                            \n",
      "tprp                          \n",
      "fprp                          \n",
      "erp                           \n"
     ]
    }
   ],
   "source": [
    "print('Black:')\n",
    "p_race_mwu(dfs_b,'B')\n",
    "\n",
    "print('\\nWhite:')\n",
    "p_race_mwu(dfs_w,'W')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}