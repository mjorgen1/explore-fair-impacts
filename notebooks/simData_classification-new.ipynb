{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. IMPORTANT--Specify classifier to be trained and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from impt_functions import *\n",
    "from visualizations import *\n",
    "from evaluation import *\n",
    "import csv\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, EqualizedOdds, \\\n",
    "    TruePositiveRateParity, FalsePositiveRateParity, ErrorRateParity, BoundedGroupLoss\n",
    "from fairlearn.metrics import *\n",
    "from raiwidgets import FairnessDashboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Decision Tree': 'dt', 'Gaussian Naive Bayes':'gnb','Logistic Regression': 'lgr', 'Gradient_Boosted_Trees': 'gbt'}\n",
    "constraints = {'DemograficParity': 'DP', 'DemograficParity': 'EO', 'DemograficParity': 'EOO', 'DemograficParity': 'FPRP', 'DemograficParity': 'ERP'}\n",
    "reduction_algorithms = {'Exponential Gradient':'EG','Grid Search':'GS'}\n",
    "\n",
    "data_path = '../data/final/simData_oom100.csv'  # ...oom10, ...oom50, ...oom100\n",
    "results_path = '../data/results/notebook/'\n",
    "save = True\n",
    "try:\n",
    "    os.mkdir('../data/results/notebook')\n",
    "except:\n",
    "  print(\"Folder exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "overall_results_dict = {}\n",
    "black_results_dict = {}\n",
    "white_results_dict = {}\n",
    "all_scores = []\n",
    "scores_names = []\n",
    "#scores_fieldnames = ['testB', 'testW', 'unmitB', 'unmitW', 'egdpB', 'egdpW', 'egeoB', 'egeoW', 'egeooB', 'egeooW', 'egfprpB', 'egfprpW', 'egerpB', 'egerpW', 'gsdpB', 'gsdpW', 'gseoB', 'gseoW', 'gseooB', 'gseooW', 'gsfprpB', 'gsfprpW', 'gserpB', 'gserpW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= models['Gradient_Boosted_Trees']\n",
    "try:\n",
    "    os.mkdir(f'../data/results/notebook/{model_name}')\n",
    "except:\n",
    "    print(\"Folder exists\")\n",
    "\n",
    "constraint = constraints['DemograficParity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, race_train, race_test, sample_weight_train, sample_weight_test = prep_data(data=data, test_size=0.3, weight_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up X_test by race\n",
    "X_test_b = []\n",
    "\n",
    "X_test_w = []\n",
    "\n",
    "for index in range(len(X_test)):\n",
    "    if race_test[index] == 0:  # black\n",
    "        X_test_b.append(X_test[index][0])\n",
    "    elif race_test[index] == 1:  # white\n",
    "        X_test_w.append(X_test[index][0])\n",
    "\n",
    "# given predictions+outcomes, I'll need to do the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_test_b,X_test_w])\n",
    "scores_names.extend(['testB', 'testW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram of credit scores by race\n",
    "visual_scores_by_race(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_repay_dist(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The classifier trained below is: ', model_name)\n",
    "\n",
    "results_path += f'{model_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier and collect predictions\n",
    "NOTE: atm sample_weight are all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "np.random.seed(0)\n",
    "\n",
    "# Train the classifier:\n",
    "model = classifier.fit(X_train,y_train, sample_weight_train)\n",
    "\n",
    "# Make predictions with the classifier:\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# Scores on test set\n",
    "test_scores = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think: later on I might want to create a sheet just with the TP and FP scores by race specifically\n",
    "X_unmit_b, X_unmit_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_unmit_b,X_unmit_w])\n",
    "scores_names.extend(['unmitB', 'unmitW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of classifier overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (y_predict, test_scores)}\n",
    "get_metrics_df(models_dict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['score', 'race']].values\n",
    "y = data['repay_indices'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, x, y, cv=5, scoring='f1_weighted')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed impact calculated\n",
    "### Fairness Metric Evaluation of classifier\n",
    "### Evaluation of classifier by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_str = 'Un-'\n",
    "results_overall, results_black, results_white = evaluating_model(constraint_str,X_test,y_test, y_predict, sample_weight_test,race_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} Unmitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentiated Gradient Reduction Alg for Adding Fairness Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 60em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'DP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get new scores by race\n",
    "X_egDP_b, X_egDP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_egDP_b, X_egDP_w])\n",
    "scores_names.extend(['egdpB', 'egdpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EG DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'EO', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egEO_b, X_egEO_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_egEO_b, X_egEO_w])\n",
    "scores_names.extend(['egeoB', 'egeoW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EG EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOO (True Positive Rate Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'TPRP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egEOO_b, X_egEOO_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_egEOO_b, X_egEOO_w])\n",
    "scores_names.extend(['egeooB', 'egeooW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EG EOO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'FPRP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egFPRP_b, X_egFPRP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_egFPRP_b, X_egFPRP_w])\n",
    "scores_names.extend(['egfprpB', 'egfprpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EG FPRP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'ERP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egERP_b, X_egERP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_egERP_b, X_egERP_w])\n",
    "scores_names.extend(['egerpB', 'egerpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_key = f'{model_name} EG ERP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounded Group Loss (issue, need to figure out loss parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'BGL', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False)\n",
    "run_key = f'{model_name} EG DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Reduction Alg for Adding Fairness Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'DP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells come from: https://github.com/fairlearn/fairlearn/blob/main/notebooks/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb\n",
    "\n",
    "Note: we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, demographic_parity_difference, y_predict, X_test, y_test, race_test, 'DemParityDifference','GS DPD', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS DPD')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsDP_b, X_gsDP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_gsDP_b, X_gsDP_w ])\n",
    "scores_names.extend(['gsdpB', 'gsdpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} GS DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'EO', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, equalized_odds_difference, y_predict, X_test, y_test, race_test, 'EOddsDifference','GS EO', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS EO')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsEO_b, X_gsEO_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_gsEO_b, X_gsEO_w])\n",
    "scores_names.extend(['gseoB', 'gseoW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} GS EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOO (True Positive Rate Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'TPRP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, true_positive_rate_difference, y_predict, X_test, y_test, race_test, 'TPRPDifference','GS TPRP', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS TPRP')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsEOO_b, X_gsEOO_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_gsEOO_b, X_gsEOO_w])\n",
    "scores_names.extend(['gseooB', 'gseooW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} GS EOO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'FPRP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the below models are the same for DT classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, false_positive_rate_difference, y_predict, X_test, y_test, race_test, 'FPRPDifference','GS FPRP', models_dict, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS FPRP')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsFPRP_b, X_gsFPRP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_gsFPRP_b, X_gsFPRP_w])\n",
    "scores_names.extend(['gsfprpB', 'gsfprpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} GS FPRP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white, y_pred_mitigated = add_constraint(model, 'ERP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairlearn doesnt have an erp difference metric for the below\n",
    "#grid_search_show(gs_erp, error_difference, y_predict, X_test, y_test, race_test, 'ERDifference','GS ERP', models_dict, 0.3)\n",
    "#models_dict.pop('GS FPRP')\n",
    "#models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scores by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsERP_b, X_gsERP_w = get_new_scores(X_test, y_predict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding scores to list\n",
    "all_scores.extend([X_gsERP_b, X_gsERP_w])\n",
    "scores_names.extend(['gserpB', 'gserpW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = f'{model_name} GS ERP Mitigated'\n",
    "print(run_key)\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounded Group Loss (issue, need to figure out loss parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'BGL', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False)\n",
    "run_key = f'{model_name} GS EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "#lambda_vecs = gs_dp.lambda_vecs_\n",
    "#print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save results to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use below!!\n",
    "if save == True:\n",
    "    overall_fieldnames = ['Run', 'Acc', 'F1micro/F1w/F1bsr', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DIB/DIW', 'DP Diff', 'EO Diff', 'TPR Diff', 'FPR Diff', 'ER Diff']\n",
    "    byrace_fieldnames = ['Run', 'Acc', 'F1micro/F1w/F1bsr', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DI']\n",
    "    save_dict_in_csv(overall_results_dict, overall_fieldnames,  results_path+model_name+'_overall_results.csv')\n",
    "    save_dict_in_csv(black_results_dict, byrace_fieldnames,  results_path+model_name+'_black_results.csv')\n",
    "    save_dict_in_csv(white_results_dict, byrace_fieldnames,  results_path+model_name+'_white_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    # Save overall score results\n",
    "    columns_data = zip_longest(*all_scores)\n",
    "\n",
    "    with open(results_path+model_name+'_overall_scores.csv',mode='w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(scores_names)\n",
    "        writer.writerows(columns_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
