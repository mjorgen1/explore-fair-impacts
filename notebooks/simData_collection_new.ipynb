{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and parse the data\n",
    "#### Code is primarily from Lydia's FICO-figures.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# import all of our files\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import Liu_paper_code.fico as fico\n",
    "import Liu_paper_code.distribution_to_loans_outcomes as dlo\n",
    "from scripts.data_creation_utils import get_pmf,get_repay_probabilities,get_scores, adjust_set_ratios\n",
    "from scripts.evaluation_utils import inspect_MinMax\n",
    "from scripts.visualization_utils import visualize_data_distribution, visual_scores_by_race, visual_repay_dist\n",
    "# imports for my own code\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import choices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_dir = '../data/raw/'\n",
    "results_dir = '../data/tsting_timeit/'\n",
    "file_name = 'test.csv'\n",
    "set_size = 1\n",
    "\n",
    "group_size_ratio = [0.12,0.88]\n",
    "black_label_ratio = [0.66,0.34]\n",
    "\n",
    "order_of_magnitude = 100000\n",
    "shuffle_seed = 42\n",
    "round_num_scores = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cdfs, performance, totals = fico.get_FICO_data(data_dir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfs = all_cdfs[[\"White\",\"Black\"]]\n",
    "\n",
    "# B is White\n",
    "# A is Black\n",
    "\n",
    "cdf_B = cdfs['White'].values\n",
    "cdf_A = cdfs['Black'].values\n",
    "\n",
    "repay_B = performance['White']\n",
    "repay_A = performance['Black']\n",
    "scores = cdfs.index\n",
    "scores_list = scores.tolist()\n",
    "scores_repay = cdfs.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "N_scores = cdf_B.size\n",
    "N_groups = 2\n",
    "\n",
    "# get probability mass functions of each group\n",
    "pi_A = get_pmf(cdf_A)\n",
    "pi_B = get_pmf(cdf_B)\n",
    "pis = np.vstack([pi_A, pi_B])\n",
    "\n",
    "# demographic statistics \n",
    "#group_ratio = np.array((totals[\"Black\"], totals[\"White\"]))\n",
    "#group_size_ratio = group_ratio/group_ratio.sum() - true fico data goup size ratio\n",
    "#print(group_size_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get loan repay probabilities for a given score\n",
    "loan_repaid_probs = [lambda i: repay_A[scores[scores.get_loc(i,method='nearest')]], \n",
    "                     lambda i: repay_B[scores[scores.get_loc(i,method='nearest')]]]\n",
    "\n",
    "# unpacking repay probability as a function of score\n",
    "loan_repay_fns = [lambda x: loan_repaid_prob(x) for\n",
    "                      loan_repaid_prob in loan_repaid_probs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Convert the data in format needed\n",
    "The parameter round_num designates if we will round the credit scores and/or probabilities to a certain degree.<br> \n",
    "round_num = {0,1,2} <br> \n",
    "* 0: don't round at all (we don't recommend this) <br>\n",
    "* 1: round to the hundreth decimal <br>\n",
    "* 2: round to the nearest integer (no decimals left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300. 312. 324. 336. 348. 355. 361. 367. 373. 379. 385. 390. 396. 402.\n",
      " 406. 411. 416. 420. 425. 430. 434. 439. 444. 448. 452. 456. 460. 464.\n",
      " 468. 472. 475. 479. 483. 487. 491. 495. 498. 502. 505. 508. 511. 515.\n",
      " 518. 521. 524. 527. 530. 534. 537. 540. 543. 546. 549. 552. 555. 557.\n",
      " 560. 562. 565. 568. 570. 573. 576. 578. 581. 583. 586. 589. 591. 594.\n",
      " 596. 599. 601. 603. 605. 608. 610. 612. 614. 616. 618. 620. 622. 624.\n",
      " 626. 628. 630. 632. 635. 637. 639. 641. 643. 645. 647. 649. 651. 653.\n",
      " 655. 657. 658. 660. 662. 664. 666. 667. 669. 671. 673. 675. 676. 678.\n",
      " 680. 682. 684. 686. 687. 689. 691. 693. 695. 696. 698. 700. 701. 703.\n",
      " 704. 706. 707. 709. 710. 712. 713. 715. 716. 718. 719. 721. 722. 724.\n",
      " 725. 726. 728. 729. 731. 734. 735. 737. 738. 740. 741. 743. 744. 746.\n",
      " 749. 750. 752. 753. 755. 756. 758. 759. 761. 763. 764. 766. 767. 769.\n",
      " 771. 772. 774. 775. 777. 778. 780. 782. 783. 785. 786. 788. 790. 791.\n",
      " 793. 796. 797. 799. 802. 806. 811. 815. 819. 824. 828. 832. 837. 841.\n",
      " 846. 850.]\n"
     ]
    }
   ],
   "source": [
    "# Make repay probabilities into percentages from decimals\n",
    "# NOTE: A is Black, B is White\n",
    "scores_arr = np.asarray(get_scores(scores=scores_list, round_num=round_num_scores)) # we recommend 1 or 2 for round_num\n",
    "print(scores_arr)\n",
    "repay_A_arr = pd.Series.to_numpy(repay_A)*100\n",
    "repay_B_arr = pd.Series.to_numpy(repay_B)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sample from the data and Combine the scores and probabilities and convert data types\n",
    "We use round_num with a value of 1 so that we have two decimals for the probabilities. We think rounding to the nearest integer would lead to us losing important data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(group_size_ratio, order_of_magnitude, shuffle_seed,scores_arr, pi_A, pi_B, repay_A_arr, repay_B_arr):\n",
    "    \n",
    "        # Sample data according to the pmf\n",
    "    # Reference: https://www.w3schools.com/python/ref_random_choices.asp\n",
    "\n",
    "    num_A_samples = int(group_size_ratio[0] * order_of_magnitude)\n",
    "    num_B_samples = int(group_size_ratio[1] * order_of_magnitude)\n",
    "\n",
    "    samples_A = np.asarray(sorted(choices(scores_arr, pi_A, k=num_A_samples)))\n",
    "    samples_B = np.asarray(sorted(choices(scores_arr, pi_B, k=num_B_samples)))\n",
    "\n",
    "    # Calculate samples groups' probabilities and make arrays for race\n",
    "\n",
    "    # A == Black == 0 (later defined as 0.0 when converting to pandas df)\n",
    "    samples_A_probs = get_repay_probabilities(samples=samples_A,scores_arr=scores_arr, repay_probs=repay_A_arr, round_num=1)\n",
    "    samples_A_race = np.zeros(num_A_samples, dtype= int)\n",
    "    # B == White == 1 (later defined as 1.0 when converting to pandas df)\n",
    "    samples_B_probs = get_repay_probabilities(samples=samples_B,scores_arr=scores_arr, repay_probs=repay_B_arr, round_num=1)\n",
    "    samples_B_race = np.ones(num_B_samples, dtype= int)\n",
    "\n",
    "    # Get data in dict form with score and repay prob\n",
    "    data_A_dict = {'score': samples_A, 'repay_probability': samples_A_probs} #,'race': samples_A_race}\n",
    "    data_B_dict = {'score': samples_B, 'repay_probability': samples_B_probs} #,'race': samples_B_race}\n",
    "\n",
    "    # Get data in dict form with score, repay prob, and race\n",
    "    data_A_dict = {'score': samples_A, 'repay_probability': samples_A_probs ,'race': samples_A_race}\n",
    "    data_B_dict = {'score': samples_B, 'repay_probability': samples_B_probs,'race': samples_B_race}\n",
    "\n",
    "    # Convert from dict to df\n",
    "    data_A_df = pd.DataFrame(data=data_A_dict, dtype=np.float64)\n",
    "    data_B_df = pd.DataFrame(data=data_B_dict, dtype=np.float64)\n",
    "\n",
    "    # Combine all of the data together and shuffle\n",
    "    # NOTE: not currently being used but could be useful at a later time\n",
    "    data_all_df = pd.concat([data_A_df, data_B_df], ignore_index=True)\n",
    "    #print(data_all_df)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    data_all_df_shuffled = data_all_df.sample(frac=1).reset_index(drop=True)\n",
    "    #print(data_all_df_shuffled)\n",
    "\n",
    "    # Add Final Column to dataframe, repay indices\n",
    "    # repay: 1.0, default: 0.0\n",
    "    probabilities = data_all_df_shuffled['repay_probability']\n",
    "    repay_indices = []\n",
    "    # Create a random num and then have that decide given a prob if the person gets a loan or not\n",
    "    # (e.g. If 80% prob, then calculate a random num, then if that is below they will get loan, if above, then they don't)\n",
    "\n",
    "    for index, prob in enumerate(probabilities):\n",
    "        rand_num = random.randint(0,1000)/10\n",
    "        if rand_num > prob:  # default\n",
    "            repay_indices.append(0)\n",
    "        else:\n",
    "            repay_indices.append(1)  # repay\n",
    "\n",
    "    data_all_df_shuffled['repay_indices'] = np.array(repay_indices)\n",
    "\n",
    "    return data_all_df_shuffled, samples_A, samples_B, samples_A_probs, samples_B_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m samples_A_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((samples_A_probs,samples_A_probs_add))\n\u001b[0;32m     31\u001b[0m samples_B \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((samples_B,samples_B_add))\n\u001b[1;32m---> 32\u001b[0m samples_B_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples_B_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43msamples_B_probs_add\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# split the data cols (x,y)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepay_probability\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample data according to the pmf\n",
    "# Reference: https://www.w3schools.com/python/ref_random_choices.asp\n",
    "# Calculate samples groups' probabilities and make arrays for race\n",
    "# A == Black == 0 (later defined as 0.0 when converting to pandas df)\n",
    "# B == White == 1 (later defined as 1.0 when converting to pandas df)\n",
    "\n",
    "# generate first batch of samples:\n",
    "data,samples_A, samples_B, samples_A_probs, samples_B_probs = sample(group_size_ratio, order_of_magnitude,shuffle_seed, scores_arr, pi_A, pi_B, repay_A_arr, repay_B_arr)\n",
    "# split the data cols (x,y)\n",
    "x = data[['score','repay_probability', 'race']].values\n",
    "y = data['repay_indices'].values\n",
    "\n",
    "# adjust the set according to the ratios specified\n",
    "x,y = adjust_set_ratios(x, y, black_label_ratio, group_size_ratio, set_size)\n",
    "idx_An = np.where((x[:, 2] == 0) & (y == 0))[0]\n",
    "idx_Ap = np.where((x[:, 2] == 0) & (y == 1))[0]\n",
    "idx_B = np.where((x[:, 2] == 1))[0]\n",
    "i = 1\n",
    "# merge x,y back into a DataFrame\n",
    "df = {'score':x[:,0],'repay_probability': x[:,1],'race':x[:,2],'repay_indices': y}\n",
    "data = pd.DataFrame(df)\n",
    "\n",
    "# if dataset it to small, samplee a larger batch\n",
    "while len(y) < set_size:\n",
    "    i += 1\n",
    "    # Generate new samples\n",
    "    data_add, samples_A_add, samples_B_add, samples_A_probs_add, samples_B_probs_add = sample(group_size_ratio, order_of_magnitude,i, scores_arr, pi_A, pi_B, repay_A_arr, repay_B_arr)\n",
    "    data = pd.concat([data,data_add])\n",
    "    samples_A = np.concatenate((samples_A,samples_A_add))\n",
    "    samples_A_probs = np.concatenate((samples_A_probs,samples_A_probs_add))\n",
    "    samples_B = np.concatenate((samples_B,samples_B_add))\n",
    "    samples_B_probs = np.concatenate((samples_B_probs,samples_B_probs_add))\n",
    "    # split the data cols (x,y)\n",
    "    x = data[['score','repay_probability', 'race']].values\n",
    "    y = data['repay_indices'].values\n",
    "\n",
    "    # adjust the set according to the ratios specified\n",
    "    x,y = adjust_set_ratios(x,y, black_label_ratio, group_size_ratio, set_size)\n",
    "    idx_An = np.where((x[:, 2] == 0) & (y == 0))[0]\n",
    "    idx_Ap = np.where((x[:, 2] == 0) & (y == 1))[0]\n",
    "    idx_B = np.where((x[:, 2] == 1))[0]\n",
    "    # merge x,y back into a DataFrame\n",
    "    df = {'score':x[:,0],'repay_probability': x[:,1],'race':x[:,2],'repay_indices': y}\n",
    "    data = pd.DataFrame(df)\n",
    "\n",
    "# print proportions of dataset\n",
    "print(i,'Black N/P:',len(idx_An),'/',len(idx_Ap),'White:',len(idx_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Save the pandas dataframes to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(index=False, path_or_buf=results_dir+file_name)\n",
    "\n",
    "# To save the data separately by race\n",
    "#data_A_df.to_csv(index=False, path_or_buf='simData_2decProbs_0decScores_groupA_black.csv')\n",
    "#data_B_df.to_csv(index=False, path_or_buf='simData_2decProbs_0decScores_groupB_white.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Inspect the min/max values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a function for printing the min/max values of the respective groups for score and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_MinMax(samples_A_probs,samples_B_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "total_samples_A = len(samples_A)\n",
    "total_samples_B = len(samples_B)\n",
    "values_A = set(samples_A)\n",
    "values_B = set(samples_B)\n",
    "res_A = Counter(samples_A)\n",
    "res_B = Counter(samples_B)\n",
    "\n",
    "pmf_A = []\n",
    "pmf_B = []\n",
    "for v_A,v_B in zip(values_A,values_B):\n",
    "    pmf_A.append(res_A[v_A]/total_samples_A)\n",
    "    pmf_B.append(res_B[v_B]/total_samples_B)\n",
    "    \n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for e1,e2 in zip(pmf_A,pmf_B):\n",
    "    if e1 < 1/550:\n",
    "        count1 += 1\n",
    "    if e2 < 1/550:\n",
    "        count2 += 1\n",
    "    \n",
    "print('Ratio:',count1/len(pmf_A), count2/len(pmf_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masuring the amount of points <1%\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for e1,e2 in zip(samples_A_probs,samples_B_probs):\n",
    "    if e1 < 1:\n",
    "        count1 += 1\n",
    "    if e2 < 1:\n",
    "        count2 += 1\n",
    "    \n",
    "print('Ratio:',count1/len(samples_A_probs), count2/len(samples_B_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_data_distribution(results_dir,samples_A,samples_A_probs,samples_B,samples_B_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_scores_by_race(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_repay_dist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
