{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/results/test_notbalanced_and_balanced/'\n",
    "#folders= ['dt','gnb','lgr','gbt']\n",
    "folders= ['dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_csvs(data_path= 'data/results/',b_or_w = 'Black', folders= ['dt','gnb','lgr','gbt']):\n",
    "\n",
    "    col_names_eg = []\n",
    "    col_names_gs = []\n",
    "\n",
    "    for i,f in enumerate(folders):\n",
    "        if b_or_w == 'Black':\n",
    "            path = f'{data_path}{f}/{f}_black_results.csv'\n",
    "        else:\n",
    "            path = f'{data_path}{f}/{f}_white_results.csv'\n",
    "\n",
    "        df = pd.read_csv(path,index_col=0)\n",
    "        df = df.reset_index()\n",
    "\n",
    "        col_names_eg.append(f'EG+{f.upper()}')\n",
    "        col_names_gs.append(f'GS+{f.upper()}')\n",
    "\n",
    "        if i == 0:\n",
    "            joined_df = df.iloc[:,-1]\n",
    "        else:\n",
    "            joined_df = pd.concat([joined_df, df.iloc[:,-1]], axis=1)\n",
    "\n",
    "    joined_df.set_axis(folders, axis=1)\n",
    "\n",
    "\n",
    "    # split dataframe after the two reduction algorithms\n",
    "    df_eg = joined_df.iloc[:6,:]\n",
    "    df_gs = pd.concat([joined_df.iloc[0:1,:],joined_df.iloc[6:,:]])\n",
    "\n",
    "    # set new index\n",
    "    df_eg['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
    "    df_eg.set_index('Constraint',inplace=True)\n",
    "    df_gs['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
    "    df_gs.set_index('Constraint',inplace=True)\n",
    "    df_eg.columns = col_names_eg\n",
    "    df_gs.columns = col_names_gs\n",
    "\n",
    "    df_final = pd.concat([df_eg, df_gs], axis=1)\n",
    "    print('Group: ',b_or_w,'\\n DataFrame: \\n',df_final)\n",
    "    print('A')\n",
    "    df_final.to_csv(f'{data_path}/{b_or_w}_DI.csv')\n",
    "    print('B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/results/test_notbalanced_and_balanced/gnb/gnb_black_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mimpact_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgnb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlgr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgbt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m impact_csvs(data_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhite\u001b[39m\u001b[38;5;124m'\u001b[39m, folders\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgnb\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgr\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mimpact_csvs\u001b[1;34m(data_path, b_or_w, folders)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_white_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     15\u001b[0m col_names_eg\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEG+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/results/test_notbalanced_and_balanced/gnb/gnb_black_results.csv'"
     ]
    }
   ],
   "source": [
    "impact_csvs(data_path,'Black', folders= ['dt','gnb','lgr','gbt'])\n",
    "impact_csvs(data_path,'White', folders= ['dt','gnb','lgr','gbt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FP/TP/TN/FN Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  dt \n",
      " DataFrame: \n",
      " ID        egdpB  egdpW  egeoB  egeoW  egerpB  egerpW  egfprpB  egfprpW  \\\n",
      "Category                                                                 \n",
      "FN        0.012  0.035  0.058  0.051   0.143   0.044    0.062    0.035   \n",
      "FP        0.331  0.129  0.131  0.122   0.045   0.153    0.121    0.129   \n",
      "TN        0.169  0.371  0.369  0.378   0.455   0.347    0.379    0.371   \n",
      "TP        0.488  0.465  0.442  0.449   0.357   0.456    0.438    0.465   \n",
      "\n",
      "ID        egtprpB  egtprpW  gsdpB  gsdpW  gseoB  gseoW  gserpB  gserpW  \\\n",
      "Category                                                                 \n",
      "FN          0.045    0.035    0.0  0.051  0.143  0.029   0.143   0.029   \n",
      "FP          0.189    0.128    0.5  0.098  0.045  0.145   0.045   0.145   \n",
      "TN          0.311    0.372    0.0  0.402  0.455  0.355   0.455   0.355   \n",
      "TP          0.455    0.465    0.5  0.449  0.357  0.471   0.357   0.471   \n",
      "\n",
      "ID        gsfprpB  gsfprpW  gstprpB  gstprpW  testB  testW  unmitB  unmitW  \n",
      "Category                                                                    \n",
      "FN          0.143    0.029    0.021    0.051    0.0    0.0   0.143   0.029  \n",
      "FP          0.045    0.145    0.269    0.098    0.0    0.0   0.045   0.145  \n",
      "TN          0.455    0.355    0.231    0.402    0.5    0.5   0.455   0.355  \n",
      "TP          0.357    0.471    0.479    0.449    0.5    0.5   0.357   0.471  \n"
     ]
    }
   ],
   "source": [
    "# Ratios\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts(normalize=True)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Ratio'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Ratio']\n",
    "    df = df.fillna(0)\n",
    "    df = df.round(decimals = 3)\n",
    "    print('Classifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_ratios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  dt \n",
      " DataFrame: \n",
      " ID         egdpB   egdpW   egeoB   egeoW  egerpB  egerpW  egfprpB  egfprpW  \\\n",
      "Category                                                                     \n",
      "FN          28.0   439.0   140.0   636.0   344.0   553.0    150.0    439.0   \n",
      "FP         799.0  1608.0   315.0  1522.0   109.0  1907.0    293.0   1608.0   \n",
      "TN         407.0  4622.0   891.0  4708.0  1097.0  4323.0    913.0   4622.0   \n",
      "TP        1178.0  5791.0  1066.0  5594.0   862.0  5677.0   1056.0   5791.0   \n",
      "\n",
      "ID        egtprpB  egtprpW   gsdpB   gsdpW   gseoB   gseoW  gserpB  gserpW  \\\n",
      "Category                                                                     \n",
      "FN          109.0    442.0     0.0   637.0   344.0   364.0   344.0   364.0   \n",
      "FP          456.0   1595.0  1206.0  1218.0   109.0  1804.0   109.0  1804.0   \n",
      "TN          750.0   4635.0     0.0  5012.0  1097.0  4426.0  1097.0  4426.0   \n",
      "TP         1097.0   5788.0  1206.0  5593.0   862.0  5866.0   862.0  5866.0   \n",
      "\n",
      "ID        gsfprpB  gsfprpW  gstprpB  gstprpW   testB   testW  unmitB  unmitW  \n",
      "Category                                                                      \n",
      "FN          344.0    364.0     50.0    637.0     0.0     0.0   344.0   364.0  \n",
      "FP          109.0   1804.0    649.0   1218.0     0.0     0.0   109.0  1804.0  \n",
      "TN         1097.0   4426.0    557.0   5012.0  1206.0  6230.0  1097.0  4426.0  \n",
      "TP          862.0   5866.0   1156.0   5593.0  1206.0  6230.0   862.0  5866.0  \n"
     ]
    }
   ],
   "source": [
    "# Absolute numbers\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Number'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Number']\n",
    "    df = df.fillna(0)\n",
    "    print('Classifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_absolute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extractig Scores from csv into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       testB  testW  unmitB  unmitW  egdpB  egdpW  egeoB  egeoW  egtprpB  \\\n",
      "0      630.0    645   705.0     720  705.0    720  705.0    720    705.0   \n",
      "1      565.0    764   415.0     839  415.0    839  415.0    839    415.0   \n",
      "2      468.0    799   468.0     850  318.0    850  468.0    850    468.0   \n",
      "3      385.0    635   385.0     710  385.0    710  385.0    635    385.0   \n",
      "4      560.0    752   560.0     827  410.0    827  410.0    827    410.0   \n",
      "...      ...    ...     ...     ...    ...    ...    ...    ...      ...   \n",
      "12455    NaN    530     NaN     530    NaN    530    NaN    530      NaN   \n",
      "12456    NaN    361     NaN     361    NaN    361    NaN    361      NaN   \n",
      "12457    NaN    527     NaN     527    NaN    527    NaN    527      NaN   \n",
      "12458    NaN    444     NaN     444    NaN    444    NaN    444      NaN   \n",
      "12459    NaN    430     NaN     430    NaN    430    NaN    430      NaN   \n",
      "\n",
      "       egtprpW  egfprpB  egfprpW  egerpB  egerpW  gsdpB  gsdpW  gseoB  gseoW  \\\n",
      "0          720    705.0      720   705.0     720  705.0    720  705.0    720   \n",
      "1          839    415.0      839   415.0     839  415.0    839  415.0    839   \n",
      "2          850    468.0      850   468.0     850  318.0    850  468.0    850   \n",
      "3          710    385.0      710   385.0     710  300.0    710  385.0    710   \n",
      "4          827    410.0      827   560.0     827  410.0    827  560.0    827   \n",
      "...        ...      ...      ...     ...     ...    ...    ...    ...    ...   \n",
      "12455      530      NaN      530     NaN     530    NaN    530    NaN    530   \n",
      "12456      361      NaN      361     NaN     361    NaN    361    NaN    361   \n",
      "12457      527      NaN      527     NaN     527    NaN    527    NaN    527   \n",
      "12458      444      NaN      444     NaN     444    NaN    444    NaN    444   \n",
      "12459      430      NaN      430     NaN     430    NaN    430    NaN    430   \n",
      "\n",
      "       gstprpB  gstprpW  gsfprpB  gsfprpW  gserpB  gserpW  \n",
      "0        705.0      720    705.0      720   705.0     720  \n",
      "1        415.0      839    415.0      839   415.0     839  \n",
      "2        318.0      850    468.0      850   468.0     850  \n",
      "3        385.0      710    385.0      710   385.0     710  \n",
      "4        410.0      827    560.0      827   560.0     827  \n",
      "...        ...      ...      ...      ...     ...     ...  \n",
      "12455      NaN      530      NaN      530     NaN     530  \n",
      "12456      NaN      361      NaN      361     NaN     361  \n",
      "12457      NaN      527      NaN      527     NaN     527  \n",
      "12458      NaN      444      NaN      444     NaN     444  \n",
      "12459      NaN      430      NaN      430     NaN     430  \n",
      "\n",
      "[12460 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Scores Data Frames\n",
    "classifier_dfs = {}\n",
    "dfs_b = {}\n",
    "dfs_w = {}\n",
    "dfs_eg = {}\n",
    "dfs_gs = {}\n",
    "for f in folders:\n",
    "    path = f'{data_path}{f}/{f}_all_scores.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    df_black = df.filter(like='B')\n",
    "    df_white = df.filter(like='W')\n",
    "    df_eg = pd.concat([df.iloc[:,:4],df.filter(like='eg')],axis=1)\n",
    "    df_gs = pd.concat([df.iloc[:,:4],df.filter(like='gs')],axis=1)\n",
    "    \n",
    "    classifier_dfs[f] = df\n",
    "    dfs_b[f] = df_black\n",
    "    dfs_w[f] = df_white\n",
    "    dfs_eg[f] = df_eg\n",
    "    dfs_gs[f] = df_gs\n",
    "print(classifier_dfs['dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if normal distributions:\n",
    "\n",
    "if p < 0.01 (or < 0.05) then the distribution is significantly different from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: dt\n",
      "Check for norm Distributions done\n"
     ]
    }
   ],
   "source": [
    "for c,df in classifier_dfs.items():\n",
    "    print('Classifier:',c)\n",
    "    for col in df:\n",
    "        data=df[col].dropna(axis=0)\n",
    "        _,p = stats.kstest(data, \"norm\")\n",
    "        if p > 0.01:\n",
    "            print(col,',p:',p)\n",
    "    print('Check for norm Distributions done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann Whitney U test:\n",
    "\n",
    "“a two-sample rank test for the difference between two population medians . . . It assumes that the data are independent random samples from two populations that have the same shape.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_path = f'{data_path}mwu/'\n",
    "os.makedirs(mwu_path,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions EG vs. GS\n",
    "if p < 0.001 (or < 0.05) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            dt\n",
      "Constraints   \n",
      "stB           \n",
      "stW           \n",
      "mitB          \n",
      "mitW          \n",
      "dpB          s\n",
      "dpW          s\n",
      "eoB          s\n",
      "eoW           \n",
      "tprpB        s\n",
      "tprpW        s\n",
      "fprpB        s\n",
      "fprpW         \n",
      "erpB          \n",
      "erpW          \n"
     ]
    }
   ],
   "source": [
    "p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "for (c1,df_eg),(c2,df_gs) in zip(dfs_eg.items(),dfs_gs.items()):\n",
    "    col_signi = []\n",
    "    col_vals = []\n",
    "    idx = []\n",
    "    \n",
    "    for col_eg,col_gs in zip(df_eg,df_gs):\n",
    "        \n",
    "        idx.append(col_eg[2:])\n",
    "        \n",
    "        data_eg=df_eg[col_eg].dropna(axis=0)\n",
    "        data_gs=df_gs[col_gs].dropna(axis=0)\n",
    "        \n",
    "        _,p = stats.mannwhitneyu(data_eg, data_gs)\n",
    "        \n",
    "        col_vals.append(p)\n",
    "        if p< 0.05:\n",
    "            col_signi.append('s')\n",
    "        else:\n",
    "            col_signi.append(' ')\n",
    "            \n",
    "    p_signi[c1] = col_signi\n",
    "    p_vals[c1] = col_vals\n",
    "    \n",
    "p_vals['Constraints'] = idx\n",
    "p_vals = p_vals.set_index('Constraints')\n",
    "p_signi['Constraints'] = idx\n",
    "p_signi = p_signi.set_index('Constraints')\n",
    "\n",
    "p_vals = p_vals.round(decimals=3)\n",
    "print(p_signi)\n",
    "    \n",
    "p_vals.to_csv(f'{mwu_path}p_eg_gs.csv')\n",
    "p_signi.to_csv(f'{mwu_path}significanz_eg_gs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions Each Model against each other\n",
    "if p < 0.001 (or < 0.05) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_values_mwu_1model(dfs,model=''):\n",
    "    p_vals = pd.DataFrame(data={'Classifier': []})\n",
    "    p_signi = pd.DataFrame(data={'Classifier': []})\n",
    "    \n",
    "    for c1,df1 in dfs.items():\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data1 = df1[model].dropna(axis=0)\n",
    "        \n",
    "        for c2,df2 in dfs.items():\n",
    "            idx.append(c2)\n",
    "            \n",
    "            data2 =df2[model].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data1, data2)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c1] = col_signi\n",
    "        p_vals[c1] = col_vals\n",
    "        \n",
    "    p_vals['Classifier'] = idx\n",
    "    p_vals = p_vals.set_index('Classifier')\n",
    "    p_signi['Classifier'] = idx\n",
    "    p_signi = p_signi.set_index('Classifier')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_{model}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_{model}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: testB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: testW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: unmitB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: unmitW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egdpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egdpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egeoB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egeoW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egtprpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egtprpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egfprpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egfprpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egerpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: egerpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gsdpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gsdpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gseoB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gseoW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gstprpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gstprpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gsfprpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gsfprpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gserpB\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n",
      "\n",
      "C: gserpW\n",
      "           dt\n",
      "Classifier   \n",
      "dt           \n"
     ]
    }
   ],
   "source": [
    "for col in classifier_dfs['dt']:\n",
    "    print('\\nC:',col)\n",
    "    p_values_mwu_1model(classifier_dfs,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions unmitigated v Mitigated for each race\n",
    "\n",
    "if p < 0.001 (or < 0.0005) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_race_mwu(dfs, b_or_w = 'B'):\n",
    "    p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "    p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "    for c,df in dfs.items():\n",
    "        \n",
    "        c = f'{c}{b_or_w}'\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data_unmiti = df[f'unmit{b_or_w}'].dropna(axis=0)\n",
    "        \n",
    "        for col in df:\n",
    "            \n",
    "            idx.append(col[:-1])\n",
    "            \n",
    "            data_miti=df[col].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data_unmiti, data_miti)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c] = col_signi\n",
    "        p_vals[c] = col_vals\n",
    "        \n",
    "    p_vals['Constraints'] = idx\n",
    "    p_vals = p_vals.set_index('Constraints')\n",
    "    \n",
    "    \n",
    "    \n",
    "    p_signi['Constraints'] = idx\n",
    "    p_signi = p_signi.set_index('Constraints')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_un_vs_miti_{b_or_w}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_un_vs_miti_{b_or_w}.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black:\n",
      "            dtB\n",
      "Constraints    \n",
      "test          s\n",
      "unmit          \n",
      "egdp          s\n",
      "egeo          s\n",
      "egtprp        s\n",
      "egfprp        s\n",
      "egerp          \n",
      "gsdp          s\n",
      "gseo           \n",
      "gstprp        s\n",
      "gsfprp         \n",
      "gserp          \n",
      "\n",
      "White:\n",
      "            dtW\n",
      "Constraints    \n",
      "test          s\n",
      "unmit          \n",
      "egdp           \n",
      "egeo           \n",
      "egtprp         \n",
      "egfprp         \n",
      "egerp          \n",
      "gsdp          s\n",
      "gseo           \n",
      "gstprp        s\n",
      "gsfprp         \n",
      "gserp          \n"
     ]
    }
   ],
   "source": [
    "print('Black:')\n",
    "p_race_mwu(dfs_b,'B')\n",
    "\n",
    "print('\\nWhite:')\n",
    "p_race_mwu(dfs_w,'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(data_unmiti, data_miti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
