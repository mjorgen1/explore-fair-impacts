{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../results/syn_raw/bi/'\n",
    "folders= ['dt','gnb','lgr','gbt']\n",
    "#folders= ['dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_csvs(data_path= 'data/results/',b_or_w = 'Black', folders= ['dt','gnb','lgr','gbt']):\n",
    "\n",
    "    col_names_df = []\n",
    "\n",
    "    for i,f in enumerate(folders):\n",
    "        if b_or_w == 'Black':\n",
    "            path = f'{data_path}{f}/{f}_black_results.csv'\n",
    "        else:\n",
    "            path = f'{data_path}{f}/{f}_white_results.csv'\n",
    "\n",
    "        df = pd.read_csv(path,index_col=0)\n",
    "        df = df.reset_index()\n",
    "\n",
    "        col_names_df.append(f'{f.upper()}')\n",
    "\n",
    "        if i == 0:\n",
    "            joined_df = df.iloc[:,-1]\n",
    "        else:\n",
    "            joined_df = pd.concat([joined_df, df.iloc[:,-1]], axis=1)\n",
    "\n",
    "    joined_df.set_axis(folders, axis=1)\n",
    "\n",
    "\n",
    "    # split dataframe after the two reduction algorithms\n",
    "    df = joined_df.iloc[:6,:]\n",
    "\n",
    "    # set new index\n",
    "    df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
    "    df.set_index('Constraint',inplace=True)\n",
    "    \n",
    "    df.columns = col_names_df\n",
    "\n",
    "    print('Group: ',b_or_w,'\\n DataFrame: \\n',df)\n",
    "    print('A')\n",
    "    df.to_csv(f'{data_path}/{b_or_w}_DI.csv')\n",
    "    print('B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  Black \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  30.99  31.74  27.82  30.99\n",
      "DP           25.61  23.90  25.07  26.49\n",
      "EO           30.41  27.63  26.23  30.39\n",
      "EOO          28.43  26.70  26.29  28.79\n",
      "FPER         30.79  27.83  27.82  30.55\n",
      "ERP          30.99  31.20  27.82  30.99\n",
      "A\n",
      "B\n",
      "Group:  White \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  38.64  35.58  38.01  38.33\n",
      "DP           39.30  38.48  39.15  39.47\n",
      "EO           37.75  36.15  35.47  37.86\n",
      "EOO          39.52  36.80  38.85  39.36\n",
      "FPER         39.09  37.52  38.01  38.99\n",
      "ERP          35.94  33.99  35.50  35.93\n",
      "A\n",
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_9752\\3506761699.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_9752\\3506761699.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n"
     ]
    }
   ],
   "source": [
    "impact_csvs(data_path,'Black', folders= ['dt','gnb','lgr','gbt'])\n",
    "impact_csvs(data_path,'White', folders= ['dt','gnb','lgr','gbt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FP/TP/TN/FN Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  dt \n",
      " DataFrame: \n",
      " ID          dpB    dpW    eoB    eoW   erpB   erpW  fprpB  fprpW  testB  \\\n",
      "Category                                                                  \n",
      "FN        0.057  0.061  0.081  0.079  0.088  0.075  0.085  0.051  0.000   \n",
      "FP        0.123  0.057  0.078  0.058  0.071  0.076  0.074  0.063  0.000   \n",
      "TN        0.215  0.185  0.259  0.184  0.266  0.166  0.263  0.179  0.337   \n",
      "TP        0.606  0.697  0.581  0.679  0.575  0.684  0.578  0.707  0.663   \n",
      "\n",
      "ID        testW  tprpB  tprpW  unmitB  unmitW  \n",
      "Category                                       \n",
      "FN        0.000  0.070  0.064   0.088   0.044  \n",
      "FP        0.000  0.097  0.054   0.071   0.069  \n",
      "TN        0.242  0.240  0.188   0.266   0.172  \n",
      "TP        0.758  0.592  0.694   0.575   0.714  \n",
      "Classifier:  gnb \n",
      " DataFrame: \n",
      " ID          dpB    dpW    eoB    eoW   erpB   erpW  fprpB  fprpW  testB  \\\n",
      "Category                                                                  \n",
      "FN        0.047  0.048  0.066  0.064  0.103  0.058  0.071  0.035  0.000   \n",
      "FP        0.139  0.069  0.107  0.078  0.062  0.095  0.101  0.081  0.000   \n",
      "TN        0.198  0.173  0.230  0.164  0.275  0.146  0.237  0.160  0.337   \n",
      "TP        0.616  0.710  0.596  0.694  0.560  0.701  0.591  0.723  0.663   \n",
      "\n",
      "ID        testW  tprpB  tprpW  unmitB  unmitW  \n",
      "Category                                       \n",
      "FN        0.000  0.059  0.058   0.107   0.023  \n",
      "FP        0.000  0.114  0.076   0.057   0.100  \n",
      "TN        0.242  0.223  0.166   0.281   0.141  \n",
      "TP        0.758  0.603  0.700   0.556   0.735  \n",
      "Classifier:  lgr \n",
      " DataFrame: \n",
      " ID          dpB    dpW    eoB    eoW   erpB   erpW  fprpB  fprpW  testB  \\\n",
      "Category                                                                  \n",
      "FN        0.052  0.056  0.061  0.058  0.067  0.083  0.067  0.036  0.000   \n",
      "FP        0.129  0.060  0.122  0.088  0.103  0.071  0.103  0.078  0.000   \n",
      "TN        0.209  0.182  0.216  0.154  0.234  0.171  0.235  0.164  0.337   \n",
      "TP        0.611  0.702  0.602  0.701  0.596  0.675  0.596  0.722  0.663   \n",
      "\n",
      "ID        testW  tprpB  tprpW  unmitB  unmitW  \n",
      "Category                                       \n",
      "FN        0.000  0.058  0.054   0.067   0.036  \n",
      "FP        0.000  0.117  0.063   0.103   0.078  \n",
      "TN        0.242  0.220  0.179   0.235   0.164  \n",
      "TP        0.758  0.604  0.704   0.596   0.722  \n",
      "Classifier:  gbt \n",
      " DataFrame: \n",
      " ID          dpB    dpW    eoB    eoW   erpB   erpW  fprpB  fprpW  testB  \\\n",
      "Category                                                                  \n",
      "FN        0.061  0.070  0.082  0.079  0.088  0.070  0.083  0.049  0.000   \n",
      "FP        0.115  0.051  0.078  0.057  0.071  0.078  0.077  0.065  0.000   \n",
      "TN        0.223  0.191  0.259  0.185  0.266  0.164  0.261  0.177  0.337   \n",
      "TP        0.602  0.689  0.580  0.679  0.575  0.688  0.580  0.709  0.663   \n",
      "\n",
      "ID        testW  tprpB  tprpW  unmitB  unmitW  \n",
      "Category                                       \n",
      "FN        0.000  0.073  0.068   0.088   0.039  \n",
      "FP        0.000  0.093  0.053   0.071   0.074  \n",
      "TN        0.242  0.244  0.189   0.266   0.168  \n",
      "TP        0.758  0.590  0.690   0.575   0.719  \n"
     ]
    }
   ],
   "source": [
    "# Ratios\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts(normalize=True)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Ratio'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Ratio']\n",
    "    df = df.fillna(0)\n",
    "    df = df.round(decimals = 3)\n",
    "    print('Classifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_ratios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  dt \n",
      " DataFrame: \n",
      " ID           dpB      dpW     eoB      eoW    erpB     erpW   fprpB    fprpW  \\\n",
      "Category                                                                       \n",
      "FN         848.0    923.0  1212.0   1195.0  1303.0   1130.0  1260.0    770.0   \n",
      "FP        1824.0    856.0  1165.0    876.0  1062.0   1144.0  1104.0    953.0   \n",
      "TN        3198.0   2798.0  3857.0   2778.0  3960.0   2510.0  3918.0   2701.0   \n",
      "TP        9014.0  10539.0  8650.0  10267.0  8559.0  10332.0  8602.0  10692.0   \n",
      "\n",
      "ID         testB    testW   tprpB    tprpW  unmitB   unmitW  \n",
      "Category                                                     \n",
      "FN           0.0      0.0  1046.0    964.0  1303.0    667.0  \n",
      "FP           0.0      0.0  1445.0    813.0  1062.0   1050.0  \n",
      "TN        5022.0   3654.0  3577.0   2841.0  3960.0   2604.0  \n",
      "TP        9862.0  11462.0  8816.0  10498.0  8559.0  10795.0  \n",
      "Classifier:  gnb \n",
      " DataFrame: \n",
      " ID           dpB      dpW     eoB      eoW    erpB     erpW   fprpB    fprpW  \\\n",
      "Category                                                                       \n",
      "FN         700.0    728.0   988.0    966.0  1534.0    871.0  1061.0    534.0   \n",
      "FP        2073.0   1036.0  1595.0   1175.0   926.0   1443.0  1497.0   1229.0   \n",
      "TN        2949.0   2618.0  3427.0   2479.0  4096.0   2211.0  3525.0   2425.0   \n",
      "TP        9162.0  10734.0  8874.0  10496.0  8328.0  10591.0  8801.0  10928.0   \n",
      "\n",
      "ID         testB    testW   tprpB    tprpW  unmitB   unmitW  \n",
      "Category                                                     \n",
      "FN           0.0      0.0   884.0    877.0  1589.0    349.0  \n",
      "FP           0.0      0.0  1698.0   1143.0   845.0   1517.0  \n",
      "TN        5022.0   3654.0  3324.0   2511.0  4177.0   2137.0  \n",
      "TP        9862.0  11462.0  8978.0  10585.0  8273.0  11113.0  \n",
      "Classifier:  lgr \n",
      " DataFrame: \n",
      " ID           dpB      dpW     eoB      eoW    erpB     erpW   fprpB    fprpW  \\\n",
      "Category                                                                       \n",
      "FN         775.0    850.0   909.0    872.0   990.0   1257.0   994.0    542.0   \n",
      "FP        1914.0    907.0  1813.0   1323.0  1533.0   1072.0  1531.0   1176.0   \n",
      "TN        3108.0   2747.0  3209.0   2331.0  3489.0   2582.0  3491.0   2478.0   \n",
      "TP        9087.0  10612.0  8953.0  10590.0  8872.0  10205.0  8868.0  10920.0   \n",
      "\n",
      "ID         testB    testW   tprpB    tprpW  unmitB   unmitW  \n",
      "Category                                                     \n",
      "FN           0.0      0.0   870.0    823.0   994.0    542.0  \n",
      "FP           0.0      0.0  1745.0    951.0  1531.0   1176.0  \n",
      "TN        5022.0   3654.0  3277.0   2703.0  3491.0   2478.0  \n",
      "TP        9862.0  11462.0  8992.0  10639.0  8868.0  10920.0  \n",
      "Classifier:  gbt \n",
      " DataFrame: \n",
      " ID           dpB      dpW     eoB      eoW    erpB     erpW   fprpB    fprpW  \\\n",
      "Category                                                                       \n",
      "FN         909.0   1051.0  1226.0   1195.0  1303.0   1061.0  1234.0    738.0   \n",
      "FP        1706.0    774.0  1160.0    865.0  1062.0   1175.0  1140.0    979.0   \n",
      "TN        3316.0   2880.0  3862.0   2789.0  3960.0   2479.0  3882.0   2675.0   \n",
      "TP        8953.0  10411.0  8636.0  10267.0  8559.0  10401.0  8628.0  10724.0   \n",
      "\n",
      "ID         testB    testW   tprpB    tprpW  unmitB   unmitW  \n",
      "Category                                                     \n",
      "FN           0.0      0.0  1087.0   1032.0  1303.0    593.0  \n",
      "FP           0.0      0.0  1389.0    795.0  1062.0   1118.0  \n",
      "TN        5022.0   3654.0  3633.0   2859.0  3960.0   2536.0  \n",
      "TP        9862.0  11462.0  8775.0  10430.0  8559.0  10869.0  \n"
     ]
    }
   ],
   "source": [
    "# Absolute numbers\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Number'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Number']\n",
    "    df = df.fillna(0)\n",
    "    print('Classifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_absolute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extractig Scores from csv into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       testB  testW  unmitB  unmitW    dpB  dpW    eoB  eoW  tprpB  tprpW  \\\n",
      "0      475.0    782   475.0     850  475.0  850  475.0  850  475.0    850   \n",
      "1      624.0    608   699.0     683  699.0  683  699.0  683  699.0    683   \n",
      "2      691.0    657   766.0     732  766.0  732  766.0  732  766.0    732   \n",
      "3      361.0    788   361.0     850  361.0  850  361.0  850  361.0    850   \n",
      "4      763.0    573   838.0     648  838.0  648  838.0  648  838.0    573   \n",
      "...      ...    ...     ...     ...    ...  ...    ...  ...    ...    ...   \n",
      "15111    NaN    576     NaN     426    NaN  426    NaN  576    NaN    576   \n",
      "15112    NaN    675     NaN     750    NaN  750    NaN  750    NaN    750   \n",
      "15113    NaN    782     NaN     850    NaN  850    NaN  850    NaN    850   \n",
      "15114    NaN    758     NaN     833    NaN  833    NaN  833    NaN    833   \n",
      "15115    NaN    537     NaN     537    NaN  537    NaN  537    NaN    537   \n",
      "\n",
      "       fprpB  fprpW   erpB  erpW  \n",
      "0      475.0    850  475.0   850  \n",
      "1      699.0    683  699.0   683  \n",
      "2      766.0    732  766.0   732  \n",
      "3      361.0    850  361.0   850  \n",
      "4      838.0    648  838.0   648  \n",
      "...      ...    ...    ...   ...  \n",
      "15111    NaN    426    NaN   426  \n",
      "15112    NaN    750    NaN   750  \n",
      "15113    NaN    850    NaN   850  \n",
      "15114    NaN    833    NaN   833  \n",
      "15115    NaN    537    NaN   537  \n",
      "\n",
      "[15116 rows x 14 columns]\n",
      "       testB  unmitB    dpB    eoB  tprpB  fprpB   erpB\n",
      "0      475.0   475.0  475.0  475.0  475.0  475.0  475.0\n",
      "1      624.0   699.0  699.0  699.0  699.0  699.0  699.0\n",
      "2      691.0   766.0  766.0  766.0  766.0  766.0  766.0\n",
      "3      361.0   361.0  361.0  361.0  361.0  361.0  361.0\n",
      "4      763.0   838.0  838.0  838.0  838.0  838.0  838.0\n",
      "...      ...     ...    ...    ...    ...    ...    ...\n",
      "15111    NaN     NaN    NaN    NaN    NaN    NaN    NaN\n",
      "15112    NaN     NaN    NaN    NaN    NaN    NaN    NaN\n",
      "15113    NaN     NaN    NaN    NaN    NaN    NaN    NaN\n",
      "15114    NaN     NaN    NaN    NaN    NaN    NaN    NaN\n",
      "15115    NaN     NaN    NaN    NaN    NaN    NaN    NaN\n",
      "\n",
      "[15116 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Scores Data Frames\n",
    "classifier_dfs = {}\n",
    "dfs_b = {}\n",
    "dfs_w = {}\n",
    "\n",
    "for f in folders:\n",
    "    path = f'{data_path}{f}/{f}_all_scores.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.round(0)\n",
    "\n",
    "    df_black = df.filter(like='B')\n",
    "    df_white = df.filter(like='W')\n",
    "    \n",
    "    classifier_dfs[f] = df\n",
    "    dfs_b[f] = df_black\n",
    "    dfs_w[f] = df_white\n",
    "\n",
    "print(classifier_dfs['dt'])\n",
    "print(dfs_b['dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if normal distributions:\n",
    "\n",
    "if p < 0.01 (or < 0.05) then the distribution is significantly different from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: dt\n",
      "Check for norm Distributions done\n",
      "Classifier: gnb\n",
      "Check for norm Distributions done\n",
      "Classifier: lgr\n",
      "Check for norm Distributions done\n",
      "Classifier: gbt\n",
      "Check for norm Distributions done\n"
     ]
    }
   ],
   "source": [
    "for c,df in classifier_dfs.items():\n",
    "    print('Classifier:',c)\n",
    "    for col in df:\n",
    "        data=df[col].dropna(axis=0)\n",
    "        _,p = stats.kstest(data, \"norm\")\n",
    "        if p > 0.01:\n",
    "            print(col,',p:',p)\n",
    "    print('Check for norm Distributions done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann Whitney U test:\n",
    "\n",
    "“a two-sample rank test for the difference between two population medians . . . It assumes that the data are independent random samples from two populations that have the same shape.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_path = f'{data_path}mwu/'\n",
    "os.makedirs(mwu_path,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions Each Model against each other\n",
    "if p < 0.001 (or < 0.05) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_values_mwu_1model(dfs,model=''):\n",
    "    p_vals = pd.DataFrame(data={'Classifier': []})\n",
    "    p_signi = pd.DataFrame(data={'Classifier': []})\n",
    "    \n",
    "    for c1,df1 in dfs.items():\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data1 = df1[model].dropna(axis=0)\n",
    "        \n",
    "        for c2,df2 in dfs.items():\n",
    "            idx.append(c2)\n",
    "            \n",
    "            data2 =df2[model].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data1, data2)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c1] = col_signi\n",
    "        p_vals[c1] = col_vals\n",
    "        \n",
    "    p_vals['Classifier'] = idx\n",
    "    p_vals = p_vals.set_index('Classifier')\n",
    "    p_signi['Classifier'] = idx\n",
    "    p_signi = p_signi.set_index('Classifier')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_{model}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_{model}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: testB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: testW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: unmitB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                  s    \n",
      "gnb                 s    \n",
      "lgr         s   s       s\n",
      "gbt                 s    \n",
      "\n",
      "C: unmitW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: dpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                     s\n",
      "lgr                      \n",
      "gbt             s        \n",
      "\n",
      "C: dpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: eoB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s   s    \n",
      "gnb         s           s\n",
      "lgr         s           s\n",
      "gbt             s   s    \n",
      "\n",
      "C: eoW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: tprpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                     s\n",
      "lgr                     s\n",
      "gbt             s   s    \n",
      "\n",
      "C: tprpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n",
      "\n",
      "C: fprpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s   s    \n",
      "gnb         s           s\n",
      "lgr         s           s\n",
      "gbt             s   s    \n",
      "\n",
      "C: fprpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: erpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                  s    \n",
      "gnb                 s    \n",
      "lgr         s   s       s\n",
      "gbt                 s    \n",
      "\n",
      "C: erpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n"
     ]
    }
   ],
   "source": [
    "for col in classifier_dfs['dt']:\n",
    "    print('\\nC:',col)\n",
    "    p_values_mwu_1model(classifier_dfs,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions unmitigated v Mitigated for each race\n",
    "\n",
    "if p < 0.001 (or < 0.0005) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_race_mwu(dfs, b_or_w = 'B'):\n",
    "    p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "    p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "    for c,df in dfs.items():\n",
    "        \n",
    "        c = f'{c}{b_or_w}'\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data_unmiti = df[f'unmit{b_or_w}'].dropna(axis=0)\n",
    "        \n",
    "        for col in df:\n",
    "            \n",
    "            idx.append(col[:-1])\n",
    "            \n",
    "            data_miti=df[col].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data_unmiti, data_miti)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c] = col_signi\n",
    "        p_vals[c] = col_vals\n",
    "        \n",
    "    p_vals['Constraints'] = idx\n",
    "    p_vals = p_vals.set_index('Constraints')\n",
    "    \n",
    "    \n",
    "    \n",
    "    p_signi['Constraints'] = idx\n",
    "    p_signi = p_signi.set_index('Constraints')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_un_vs_miti_{b_or_w}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_un_vs_miti_{b_or_w}.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black:\n",
      "            dtB gnbB lgrB gbtB\n",
      "Constraints                   \n",
      "test          s    s    s    s\n",
      "unmit                         \n",
      "dp            s    s    s    s\n",
      "eo                 s          \n",
      "tprp          s    s         s\n",
      "fprp               s          \n",
      "erp                           \n",
      "\n",
      "White:\n",
      "            dtW gnbW lgrW gbtW\n",
      "Constraints                   \n",
      "test          s    s    s    s\n",
      "unmit                         \n",
      "dp                            \n",
      "eo                            \n",
      "tprp                          \n",
      "fprp                          \n",
      "erp                     s     \n"
     ]
    }
   ],
   "source": [
    "print('Black:')\n",
    "p_race_mwu(dfs_b,'B')\n",
    "\n",
    "print('\\nWhite:')\n",
    "p_race_mwu(dfs_w,'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt\n",
      "             DP Diff  EO Diff  TPR Diff  FPR Diff  ER Diff\n",
      "Unmitigated    13.72     7.59      7.39      7.59     4.53\n",
      "DP              2.57    12.89      0.55     12.89     6.18\n",
      "EO              7.77     1.86      1.86      0.78     2.27\n",
      "TPRP            5.89     6.52      2.20      6.52     4.98\n",
      "FPRP           11.83     6.06      6.06      4.10     4.48\n",
      "ERP            11.28    10.16      3.35     10.16     0.85\n",
      "gnb\n",
      "             DP Diff  EO Diff  TPR Diff  FPR Diff  ER Diff\n",
      "Unmitigated    22.29    24.69     13.07     24.69     4.01\n",
      "DP              2.38    12.93      0.75     12.93     6.96\n",
      "EO              6.87     1.59      1.59      0.40     3.19\n",
      "TPRP            5.86     2.53      1.31      2.53     3.98\n",
      "FPRP           11.24     6.10      6.10      3.83     5.52\n",
      "ERP            17.44    21.05      7.96     21.05     1.22\n",
      "lgr\n",
      "             DP Diff  EO Diff  TPR Diff  FPR Diff  ER Diff\n",
      "Unmitigated    10.15     5.35      5.35      1.70     5.60\n",
      "DP              2.29    13.29      0.44     13.29     6.44\n",
      "EO              6.48     1.61      1.61      0.11     3.77\n",
      "TPRP            4.54     8.72      1.64      8.72     5.83\n",
      "FPRP           10.15     5.35      5.35      1.70     5.60\n",
      "ERP             4.70     1.19      0.93      1.19     1.54\n",
      "gbt\n",
      "             DP Diff  EO Diff  TPR Diff  FPR Diff  ER Diff\n",
      "Unmitigated    14.66     9.45      8.04      9.45     4.57\n",
      "DP              2.38    12.79      0.05     12.79     5.50\n",
      "EO              7.83     2.01      2.01      0.57     2.40\n",
      "TPRP            5.97     5.90      2.02      5.90     4.55\n",
      "FPRP           11.79     6.07      6.07      4.09     4.59\n",
      "ERP            11.94    11.01      3.96     11.01     1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_9752\\590112060.py:6: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df2 = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "# Absolute numbers\n",
    "dfs = {} # list for pandas dfs\n",
    "#datasets = ['00', '0b', '0i', 'b0','bb','bi','i0','ib','ii']\n",
    "#datasets = ['00', '0b']\n",
    "folders = ['dt','gnb','lgr','gbt']\n",
    "df2 = pd.Series()\n",
    "#pd.set_option('display.max_columns', None)\n",
    "for _,f in enumerate(folders):\n",
    "    print(f)\n",
    "    #for _,ds in enumerate(datasets):\n",
    "        #print(ds)\n",
    "    path = f'{data_path}{f}/{f}_overall_results.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.set_index('Run')\n",
    "    df = df.iloc[:,12:]\n",
    "    \n",
    "  #  df2 = df.iloc[:,:0]\n",
    "  #  df2['FairDiff'] = df.iloc[:,0]\n",
    "   # print(df,df2)\n",
    "   # for i in range(1,5):\n",
    "      #  df2.iloc[i,0] = df.iloc[i,i]\n",
    "\n",
    "    #print(df2)\n",
    "    df = df.set_index(pd.Index(['Unmitigated','DP','EO','TPRP','FPRP','ERP']))\n",
    "    print(df)\n",
    "    #dfs[f] = df2\n",
    "    #print('\\nClassifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}/{f}/{f}_fair_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt\n",
      "               Acc  F1weighted\n",
      "Unmitigated  86.39       86.36\n",
      "DP           85.16       84.91\n",
      "EO           85.17       85.26\n",
      "TPRP         85.77       85.71\n",
      "FPRP         86.38       86.37\n",
      "ERP          84.54       84.60\n",
      "gnb\n",
      "               Acc  F1weighted\n",
      "Unmitigated  85.67       85.56\n",
      "DP           84.88       84.35\n",
      "EO           84.25       84.01\n",
      "TPRP         84.66       84.34\n",
      "FPRP         85.60       85.28\n",
      "ERP          84.09       84.10\n",
      "lgr\n",
      "               Acc  F1weighted\n",
      "Unmitigated  85.86       85.53\n",
      "DP           85.18       84.83\n",
      "EO           83.61       83.17\n",
      "TPRP         85.37       85.09\n",
      "FPRP         85.86       85.53\n",
      "ERP          83.83       83.72\n",
      "gbt\n",
      "               Acc  F1weighted\n",
      "Unmitigated  86.41       86.35\n",
      "DP           85.20       85.06\n",
      "EO           85.18       85.28\n",
      "TPRP         85.66       85.64\n",
      "FPRP         86.36       86.33\n",
      "ERP          84.66       84.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_9752\\1834852192.py:6: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df2 = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "# Absolute numbers\n",
    "dfs = {} # list for pandas dfs\n",
    "#datasets = ['00', '0b', '0i', 'b0','bb','bi','i0','ib','ii']\n",
    "#datasets = ['00', '0b']\n",
    "folders = ['dt','gnb','lgr','gbt']\n",
    "df2 = pd.Series()\n",
    "#pd.set_option('display.max_columns', None)\n",
    "for _,f in enumerate(folders):\n",
    "    print(f)\n",
    "    #for _,ds in enumerate(datasets):\n",
    "        #print(ds)\n",
    "    path = f'{data_path}{f}/{f}_overall_results.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.set_index('Run')\n",
    "    df = pd.concat([df.iloc[:,:1],df.iloc[:,3:4]],axis=1)\n",
    "    \n",
    "  #  df2 = df.iloc[:,:0]\n",
    "  #  df2['FairDiff'] = df.iloc[:,0]\n",
    "   # print(df,df2)\n",
    "   # for i in range(1,5):\n",
    "      #  df2.iloc[i,0] = df.iloc[i,i]\n",
    "\n",
    "    #print(df2)\n",
    "    df = df.set_index(pd.Index(['Unmitigated','DP','EO','TPRP','FPRP','ERP']))\n",
    "    print(df)\n",
    "    #dfs[f] = df2\n",
    "    #print('\\nClassifier: ',f,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}/{f}/{f}_model_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats.pearsonr(data_unmiti, data_miti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "for c1,df1 in classifier_dfs.items():\n",
    "   \n",
    "    idx = []\n",
    "    \n",
    "    for col1 in df1:\n",
    "        data1=df1[col1].dropna(axis=0)\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        for c2,df2 in classifier_dfs.items():\n",
    "            \n",
    "            for col2 in df2:\n",
    "                data2=df2[col2].dropna(axis=0)\n",
    "                idx.append(f'{c2}{col2}')\n",
    "                \n",
    "                _,p = stats.mannwhitneyu(data1, data2)\n",
    "\n",
    "                col_vals.append(p)\n",
    "                if p< 0.05:\n",
    "                    col_signi.append('s')\n",
    "                else:\n",
    "                    col_signi.append(' ')\n",
    "        print(f'{c1}{col1}')\n",
    "        p_signi[f'{c1}{col1}'] = col_signi\n",
    "        p_vals[f'{c1}{col1}'] = col_vals\n",
    "idx = idx[:96]\n",
    "p_vals['Constraints'] = idx\n",
    "p_vals = p_vals.set_index('Constraints')\n",
    "p_signi['Constraints'] = idx\n",
    "p_signi = p_signi.set_index('Constraints')\n",
    "\n",
    "p_vals = p_vals.round(decimals=3)\n",
    "    \n",
    "p_vals.to_csv(f'{mwu_path}p_all.csv')\n",
    "p_signi.to_csv(f'{mwu_path}significanz_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
