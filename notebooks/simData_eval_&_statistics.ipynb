{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../results/base_distris/std15_m75,150/'\n",
    "folders= ['dt','gnb','lgr','gbt']\n",
    "#folders= ['dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_csvs(data_path= 'data/results/',b_or_w = 'Black', folders= ['dt','gnb','lgr','gbt']):\n",
    "\n",
    "    col_names_df = []\n",
    "\n",
    "    for i,f in enumerate(folders):\n",
    "        if b_or_w == 'Black':\n",
    "            path = f'{data_path}{f}/{f}_black_results.csv'\n",
    "        else:\n",
    "            path = f'{data_path}{f}/{f}_white_results.csv'\n",
    "\n",
    "        df = pd.read_csv(path,index_col=0)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        col_names_df.append(f'{f.upper()}')\n",
    "\n",
    "        if i == 0:\n",
    "            joined_df = df.iloc[:,-1]\n",
    "        else:\n",
    "            joined_df = pd.concat([joined_df, df.iloc[:,-1]], axis=1)\n",
    "\n",
    "    joined_df.set_axis(folders, axis=1)\n",
    "\n",
    "    # split dataframe after the two reduction algorithms\n",
    "    df = joined_df.iloc[:6,:]\n",
    "\n",
    "    # set new index\n",
    "    df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
    "    df.set_index('Constraint',inplace=True)\n",
    "    \n",
    "    df.columns = col_names_df\n",
    "\n",
    "    print('Group: ',b_or_w,'\\n DataFrame: \\n',df)\n",
    "    df.to_csv(f'{data_path}/{b_or_w}_DI.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  Black \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated   6.60   0.00   7.12   6.30\n",
      "DP          -40.80 -28.43 -40.68 -40.81\n",
      "EO           -5.12 -14.02  -7.76  -4.42\n",
      "EOO         -13.53 -19.32 -17.32 -14.92\n",
      "FPER         -7.17 -18.49  -8.03  -6.48\n",
      "ERP           6.54   8.39   7.11   6.57\n",
      "Group:  White \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  37.72  36.08  37.55  37.81\n",
      "DP           38.35  37.52  38.43  38.51\n",
      "EO           37.49  35.24  37.02  37.81\n",
      "EOO          38.70  38.27  38.52  38.54\n",
      "FPER         38.51  37.75  38.33  38.40\n",
      "ERP          35.78  33.86  37.63  36.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_21060\\2563935939.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_21060\\2563935939.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n"
     ]
    }
   ],
   "source": [
    "impact_csvs(data_path,'Black', folders= ['dt','gnb','lgr','gbt'])\n",
    "impact_csvs(data_path,'White', folders= ['dt','gnb','lgr','gbt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_impact_csvs(data_path= 'data/results/',b_or_w = 'Black', folders= ['dt','gnb','lgr','gbt']):\n",
    "\n",
    "    col_names_df = []\n",
    "\n",
    "    for i,f in enumerate(folders):\n",
    "        if b_or_w == 'Black':\n",
    "            path = f'{data_path}{f}/{f}_type_ratios.csv'\n",
    "            df = pd.read_csv(path,index_col=0)\n",
    "            df = df.filter(like='B')          \n",
    "\n",
    "        else:\n",
    "            path = f'{data_path}{f}/{f}_type_ratios.csv'\n",
    "            df = pd.read_csv(path,index_col=0)\n",
    "            df = df.filter(like='W')          \n",
    "        col_names_df.append(f'{f.upper()}')\n",
    "        df = df.iloc[3,[6,0,1,5,3,2]] - df.iloc[0,[6,0,1,5,3,2]]\n",
    "    \n",
    "        if i == 0:\n",
    "            joined_df = df.iloc[:]\n",
    "        else:\n",
    "            joined_df = pd.concat([joined_df, df.iloc[:]], axis=1)\n",
    "\n",
    "    joined_df.set_axis(folders, axis=1)\n",
    "    \n",
    "    # split dataframe after the two reduction algorithms\n",
    "    df = joined_df.iloc[:6,:]\n",
    "\n",
    "    # set new index\n",
    "    df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
    "    df.set_index('Constraint',inplace=True)\n",
    "    \n",
    "    df.columns = col_names_df\n",
    "\n",
    "    print('Group: ',b_or_w,'\\n DataFrame: \\n',df)\n",
    "\n",
    "    df.to_csv(f'{data_path}/{b_or_w}_FN_I.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FP/TP/TN/FN Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts(normalize=True)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Ratio'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Ratio']\n",
    "    df = df.fillna(0)\n",
    "    df = df.round(decimals = 3)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_ratios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  Black \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  0.149 -0.337  0.137  0.157\n",
      "DP           0.320  0.230  0.319  0.319\n",
      "EO           0.235  0.253  0.242  0.235\n",
      "EOO          0.259  0.279  0.275  0.264\n",
      "FPER         0.239  0.045  0.245  0.242\n",
      "ERP          0.149  0.074  0.134  0.153\n",
      "Group:  White \n",
      " DataFrame: \n",
      "                 DT    GNB    LGR    GBT\n",
      "Constraint                             \n",
      "Unmitigated  0.686  0.705  0.687  0.684\n",
      "DP           0.665  0.565  0.665  0.662\n",
      "EO           0.593  0.625  0.599  0.590\n",
      "EOO          0.644  0.667  0.656  0.650\n",
      "FPER         0.662  0.672  0.670  0.662\n",
      "ERP          0.637  0.611  0.568  0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_21060\\724994539.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n",
      "C:\\Users\\HaRi\\AppData\\Local\\Temp\\ipykernel_21060\\724994539.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Constraint'] = ['Unmitigated', 'DP', 'EO', 'EOO','FPER','ERP']\n"
     ]
    }
   ],
   "source": [
    "neg_impact_csvs(data_path,'Black', folders= ['dt','gnb','lgr','gbt'])\n",
    "neg_impact_csvs(data_path,'White', folders= ['dt','gnb','lgr','gbt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute numbers\n",
    "dfs = {} # list for pandas dfs\n",
    "pd.set_option('display.max_columns', None)\n",
    "for i,f in enumerate(folders):\n",
    "    path = f'{data_path}{f}/{f}_all_types.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.melt(var_name=\"ID\",value_name=\"Category\")\n",
    "    df = df.groupby('ID').value_counts()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns= {0:'Number'})\n",
    "    \n",
    "    df = df.pivot(index='Category', columns='ID')['Number']\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(f'{data_path}{f}/{f}_type_absolute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extractig Scores from csv into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores Data Frames\n",
    "classifier_dfs = {}\n",
    "dfs_b = {}\n",
    "dfs_w = {}\n",
    "\n",
    "for f in folders:\n",
    "    path = f'{data_path}{f}/{f}_all_scores.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.round(0)\n",
    "\n",
    "    df_black = df.filter(like='B')\n",
    "    df_white = df.filter(like='W')\n",
    "    \n",
    "    classifier_dfs[f] = df\n",
    "    dfs_b[f] = df_black\n",
    "    dfs_w[f] = df_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if normal distributions:\n",
    "\n",
    "if p < 0.01 (or < 0.05) then the distribution is significantly different from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: dt\n",
      "Check for norm Distributions done\n",
      "Classifier: gnb\n",
      "Check for norm Distributions done\n",
      "Classifier: lgr\n",
      "Check for norm Distributions done\n",
      "Classifier: gbt\n",
      "Check for norm Distributions done\n"
     ]
    }
   ],
   "source": [
    "for c,df in classifier_dfs.items():\n",
    "    print('Classifier:',c)\n",
    "    for col in df:\n",
    "        data=df[col].dropna(axis=0)\n",
    "        _,p = stats.kstest(data, \"norm\")\n",
    "        if p > 0.01:\n",
    "            print(col,',p:',p)\n",
    "    print('Check for norm Distributions done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann Whitney U test:\n",
    "\n",
    "“a two-sample rank test for the difference between two population medians . . . It assumes that the data are independent random samples from two populations that have the same shape.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_path = f'{data_path}mwu/'\n",
    "os.makedirs(mwu_path,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions Each Model against each other\n",
    "if p < 0.001 (or < 0.05) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_values_mwu_1model(dfs,model=''):\n",
    "    p_vals = pd.DataFrame(data={'Classifier': []})\n",
    "    p_signi = pd.DataFrame(data={'Classifier': []})\n",
    "    \n",
    "    for c1,df1 in dfs.items():\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data1 = df1[model].dropna(axis=0)\n",
    "        \n",
    "        for c2,df2 in dfs.items():\n",
    "            idx.append(c2)\n",
    "            \n",
    "            data2 =df2[model].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data1, data2)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c1] = col_signi\n",
    "        p_vals[c1] = col_vals\n",
    "        \n",
    "    p_vals['Classifier'] = idx\n",
    "    p_vals = p_vals.set_index('Classifier')\n",
    "    p_signi['Classifier'] = idx\n",
    "    p_signi = p_signi.set_index('Classifier')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_{model}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_{model}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: testB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: testW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: unmitB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: unmitW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: dpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n",
      "\n",
      "C: dpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: eoB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n",
      "\n",
      "C: eoW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n",
      "\n",
      "C: tprpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s   s    \n",
      "gnb         s           s\n",
      "lgr         s            \n",
      "gbt             s        \n",
      "\n",
      "C: tprpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: fprpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n",
      "\n",
      "C: fprpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: erpB\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt                       \n",
      "gnb                      \n",
      "lgr                      \n",
      "gbt                      \n",
      "\n",
      "C: erpW\n",
      "           dt gnb lgr gbt\n",
      "Classifier               \n",
      "dt              s        \n",
      "gnb         s       s   s\n",
      "lgr             s        \n",
      "gbt             s        \n"
     ]
    }
   ],
   "source": [
    "for col in classifier_dfs['dt']:\n",
    "    print('\\nC:',col)\n",
    "    p_values_mwu_1model(classifier_dfs,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Distributions unmitigated v Mitigated for each race\n",
    "\n",
    "if p < 0.001 (or < 0.0005) then the distributions are significantly different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_race_mwu(dfs, b_or_w = 'B'):\n",
    "    p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "    p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "    for c,df in dfs.items():\n",
    "        \n",
    "        c = f'{c}{b_or_w}'\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        idx = []\n",
    "        \n",
    "        data_unmiti = df[f'unmit{b_or_w}'].dropna(axis=0)\n",
    "        df = df.iloc[:,1:]\n",
    "        for col in df:\n",
    "            \n",
    "            idx.append(col[:-1])\n",
    "            \n",
    "            data_miti=df[col].dropna(axis=0)\n",
    "            \n",
    "            _,p = stats.mannwhitneyu(data_unmiti, data_miti)\n",
    "            \n",
    "            col_vals.append(p)\n",
    "            if p< 0.05:\n",
    "                col_signi.append('s')\n",
    "            else:\n",
    "                col_signi.append(' ')\n",
    "                \n",
    "        p_signi[c] = col_signi\n",
    "        p_vals[c] = col_vals\n",
    "        \n",
    "    p_vals['Constraints'] = idx\n",
    "    p_vals = p_vals.set_index('Constraints')\n",
    "    \n",
    "    \n",
    "    \n",
    "    p_signi['Constraints'] = idx\n",
    "    p_signi = p_signi.set_index('Constraints')\n",
    "    \n",
    "    p_vals = p_vals.round(decimals=3)\n",
    "    print(p_signi)\n",
    "    \n",
    "    p_vals.to_csv(f'{mwu_path}p_un_vs_miti_{b_or_w}.csv')\n",
    "    p_signi.to_csv(f'{mwu_path}significanz_un_vs_miti_{b_or_w}.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black:\n",
      "            dtB gnbB lgrB gbtB\n",
      "Constraints                   \n",
      "unmit                         \n",
      "dp            s    s    s    s\n",
      "eo            s    s    s    s\n",
      "tprp          s    s    s    s\n",
      "fprp          s    s    s    s\n",
      "erp                           \n",
      "\n",
      "White:\n",
      "            dtW gnbW lgrW gbtW\n",
      "Constraints                   \n",
      "unmit                         \n",
      "dp                            \n",
      "eo                            \n",
      "tprp                          \n",
      "fprp                          \n",
      "erp                s          \n"
     ]
    }
   ],
   "source": [
    "print('Black:')\n",
    "p_race_mwu(dfs_b,'B')\n",
    "\n",
    "print('\\nWhite:')\n",
    "p_race_mwu(dfs_w,'W')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Fairness Metrics "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Absolute numbers\n",
    "\n",
    "dataset_folder = ['00/', '0b/', '0i/', 'b0/','bb/','bi/','i0/','ib/','ii/']\n",
    "parent_path = '../results/syn_raw/'\n",
    "folders = ['dt','gnb','lgr','gbt']\n",
    "for _,ds_f in enumerate(dataset_folder):\n",
    "    for _,f in enumerate(folders):\n",
    "        print(ds_f,f)\n",
    "        path = f'{parent_path}{ds_f}{f}/{f}_overall_results.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.set_index('Run')\n",
    "        df = df.iloc[:,12:]\n",
    "        df = df.set_index(pd.Index(['Unmitigated','DP','EO','TPRP','FPRP','ERP']))\n",
    "        print(df)\n",
    "\n",
    "        df.to_csv(f'{parent_path}{ds_f}{f}/{f}_fair_performance.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Absolute numbers - one col\n",
    "\n",
    "dataset_folder = ['00', '0b', '0i', 'b0','bb','bi','i0','ib','ii']\n",
    "parent_path = '../results/syn_orig/'\n",
    "folders = ['dt']\n",
    "\n",
    "df_comb = pd.Series()\n",
    "idx = []\n",
    "for i,ds_f in enumerate(dataset_folder):\n",
    "    for _,f in enumerate(folders):\n",
    "        print(ds_f,f)\n",
    "        path = f'{parent_path}{ds_f}/{f}/{f}_overall_results.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.set_index('Run')\n",
    "        df = df.iloc[5:6,12:]\n",
    "        if i == 0:\n",
    "            df_comb = df\n",
    "            idx = [ds_f]\n",
    "        else:\n",
    "            df_comb = pd.concat([df_comb,df])\n",
    "            idx.append(ds_f)\n",
    "print(df_comb)\n",
    "df_comb = df_comb.set_index(pd.Index(idx))\n",
    "print(df_comb)\n",
    "\n",
    "df_comb.to_csv(f'{parent_path}/{f}_fair_performance.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Absolute numbers\n",
    "dataset_folder = ['00/', '0b/', '0i/', 'b0/','bb/','bi/','i0/','ib/','ii/']\n",
    "parent_path = '../results/syn_orig/'\n",
    "folders = ['dt','gnb','lgr','gbt']\n",
    "for _,ds_f in enumerate(dataset_folder):\n",
    "    for _,f in enumerate(folders):\n",
    "        print(ds_f,f)\n",
    "        path = f'{parent_path}{ds_f}{f}/{f}_overall_results.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.set_index('Run')\n",
    "        df = pd.concat([df.iloc[:,:1],df.iloc[:,3:4]],axis=1)\n",
    "        df = df.set_index(pd.Index(['Unmitigated','DP','EO','TPRP','FPRP','ERP']))\n",
    "        print(df)\n",
    "\n",
    "        df.to_csv(f'{parent_path}{ds_f}{f}/{f}_model_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "p_vals = pd.DataFrame(data={'Constraints': []})\n",
    "p_signi = pd.DataFrame(data={'Constraints': []})\n",
    "    \n",
    "for c1,df1 in classifier_dfs.items():\n",
    "   \n",
    "    idx = []\n",
    "    \n",
    "    for col1 in df1:\n",
    "        data1=df1[col1].dropna(axis=0)\n",
    "        col_signi = []\n",
    "        col_vals = []\n",
    "        for c2,df2 in classifier_dfs.items():\n",
    "            \n",
    "            for col2 in df2:\n",
    "                data2=df2[col2].dropna(axis=0)\n",
    "                idx.append(f'{c2}{col2}')\n",
    "                \n",
    "                _,p = stats.mannwhitneyu(data1, data2)\n",
    "\n",
    "                col_vals.append(p)\n",
    "                if p< 0.05:\n",
    "                    col_signi.append('s')\n",
    "                else:\n",
    "                    col_signi.append(' ')\n",
    "        print(f'{c1}{col1}')\n",
    "        p_signi[f'{c1}{col1}'] = col_signi\n",
    "        p_vals[f'{c1}{col1}'] = col_vals\n",
    "idx = idx[:96]\n",
    "p_vals['Constraints'] = idx\n",
    "p_vals = p_vals.set_index('Constraints')\n",
    "p_signi['Constraints'] = idx\n",
    "p_signi = p_signi.set_index('Constraints')\n",
    "\n",
    "p_vals = p_vals.round(decimals=3)\n",
    "    \n",
    "p_vals.to_csv(f'{mwu_path}p_all.csv')\n",
    "p_signi.to_csv(f'{mwu_path}significanz_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
